# Historical Document Analysis - Solving Real world Data Science Problems with LLMs

This Project explores the use of Large Language Models (LLMs) to analyze historical documents and extract meaningful insights.

## Setup
1. **Clone the Repository**
```bash
   git clone https://github.com/kumudhini1234/Historical_Docs_Analysis
   cd historical-docs-analysis
```

2. **Install Dependencies**
```bash
     pip install -r requirements.txt
```

3. **Setup API Keys**

Add you OpenAI API key as a kaggle secret(or environment variable)
On kagle: go to Add-ons -> Secrets and create a secret with label openai_api_key

4. **Run the Notebokk**

Open the Kaggle notebook (or Jupyter Notebook) and run through the cells to analyse the dataset.

User secrets in Kaggle: https://www.kaggle.com/discussions/product-feedback/114053 <br/>
Set up OpenAI API: https://platform.openai.com/docs/quickstart?context=python <br/>
Set up Ollama: https://github.com/ollama/ollama

## Data
The data comes from the Bureau of Refugees, Freedmen, & Abandoned Lands (also known simply as the Freedmen's Bureau), which was established by the United States government after the Civil War in 1865 to help support formerly enslaved individuals with greater access to job opportunities, education, and other resources.

Hundreds of thousands of written documents have been digitally transcribed by volunteers working with the Smithsonian Institute. A subset of these documents can be found on Kaggle for use in NLP analysis.

Here is a link to the dataset: [kaggle.com/datasets/keithgalli/freedmens-bureau-historical-documents](https://www.kaggle.com/datasets/keithgalli/freedmens-bureau-historical-documents). 

If you enjoy working on this project, it would mean a lot if you could upvote the dataset on Kaggle to help more people find the project!
